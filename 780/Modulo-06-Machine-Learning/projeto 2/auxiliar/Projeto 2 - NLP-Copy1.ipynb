{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362c03d1",
   "metadata": {},
   "source": [
    "# Projeto 2 - NLP\n",
    "\n",
    "-----\n",
    "\n",
    "Nome:  Johnny Hideki Horita <br>\n",
    "Turma: 780"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517a372",
   "metadata": {},
   "source": [
    "Os segundo projeto do m√≥dulo de Machine Learning ser√° focado no processamento de linguagem natural! Usaremos os algoritmos aprendidos e as t√©cnicas vistas na segunda parte do curso para extrairmos informa√ß√µes relevantes de texto. Mais precisamente, de publica√ß√µes no Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a3a98",
   "metadata": {},
   "source": [
    "## Os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d5657",
   "metadata": {},
   "source": [
    "Utilizaremos um Dataset obtido do Twitter com 100K postagens entre os dias 01/08/2018 e 20/10/2018. Cada postagem √© classificada como **positiva**, **negativa** ou **neutra**.  \n",
    "\n",
    "Dois arquivos ser√£o disponilizados para o desenvolvimento dos modelos, um para treino/valida√ß√£o e outro para submiss√£o. Os arquivos se encontram na pasta */Dados/train* e */Dados/subm*, respectivamente.\n",
    "\n",
    "Descri√ß√£o das colunas:\n",
    "\n",
    "- **id**: ID √∫nico para o tweet  \n",
    "- **tweet_text**: Texto da publica√ß√£o no Twitter  \n",
    "- **tweet_date**: Data da publica√ß√£o no Twitter  \n",
    "- **sentiment**: 0, se negativo; 1, se positivo; 2, se neutro  \n",
    "- **query_used**: Filtro utilizado para buscar a publica√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc86eb5",
   "metadata": {},
   "source": [
    "## O Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e1f6f",
   "metadata": {},
   "source": [
    "Voc√™ dever√° desenvolver um modelo para detectar o sentimento de uma publica√ß√£o do Twitter a classificando em uma das tr√™s categorias: **positiva**, **negativa** ou **neutra**. O texto da publica√ß√£o est√° dispon√≠vel na coluna \"tweet_text\". Teste pelo menos 3 t√©cnicas de NLP diferentes e escolha a m√©trica de avalia√ß√£o que julgar mais pertinente.  \n",
    "\n",
    "Escolha o melhor modelo e gere uma base a partir dos dados de submiss√£o, que est√£o no caminho ```Dados/subm/Subm3Classes.csv```, com o seguinte formato:\n",
    "\n",
    "\n",
    "|id|sentiment_predict\n",
    "|-|-|\n",
    "|12123232|0\n",
    "|323212|1\n",
    "|342235|2\n",
    "\n",
    "Salve essa tabela como um arquivo csv com o nome ```<nome>_<sobrenome>_nlp_degree.csv``` e submeta-o como parte da entrega final do projeto.  \n",
    "\n",
    "Para ajudar no desenvolvimento, √© poss√≠vel dividir o projeto em algumas fases:\n",
    "\n",
    "- **An√°lise de consist√™ncia dos dados**: analise se os dados est√£o fazendo sentido, se os campos est√£o completos e se h√° dados duplicados ou faltantes. Se julgar necess√°rio, trate-os.    \n",
    "\n",
    "\n",
    "- **An√°lise explorat√≥ria**: analise a sua base como um todo, verifique o balanceamento entre as classes e foque, principalmente, na coluna ```tweet_text```.    \n",
    "\n",
    "\n",
    "- **Pr√©-processamento e transforma√ß√µes**: projetos de NLP exigem um consider√°vel pr√©-processamento. Foque no tratamento da string do texto. Procure come√ßar com tratamentos simples e adicione complexidade gradualmente. Nessa etapa voc√™ testar√° diferentes t√©cnicas de transforma√ß√µes, como o Bag Of Words e o TF-IDF.    \n",
    "\n",
    "\n",
    "- **Treinamento do modelo**: depois das transforma√ß√µes, voc√™ poder√° executar o treinamento do modelo classificador. Nessa etapa o problema se torna semelhante aos abordados na primeira parte do m√≥dulo. Voc√™ pode testar diversos classificadores como RandomForest, AdaBoost, entre outros. Otimize os hiperpar√¢metros do modelo com t√©cnicas como a GridSearch e a RandomizedSearch.    \n",
    "\n",
    "\n",
    "- **Conclus√µes**: descreva, em texto, as conclus√µes sobre os seus estudos. O modelo √© capaz de identificar o sentimento das publica√ß√µes? √â poss√≠vel extrapolar o modelo para outros contextos, como a an√°lise de sentimento de uma frase qualquer? Pense em quest√µes pertinentes e relevantes que voc√™ tenha obtido durante o desenvolvimento do projeto!     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283638c9",
   "metadata": {},
   "source": [
    "## Crit√©rios de avalia√ß√£o\n",
    "\n",
    "Os seguintes itens ser√£o avaliados:\n",
    "\n",
    "1. Desenvolvimento das etapas descritas acima;\n",
    "\n",
    "\n",
    "2. Reprodutibilidade do c√≥digo: seu c√≥digo ser√° executado e precisa gerar os mesmos resultados apresentados por voc√™;\n",
    "\n",
    "\n",
    "3. Clareza: seu c√≥digo precisa ser claro e deve existir uma linha de racioc√≠nio direta. Comente o c√≥digo em pontos que julgar necess√°rio para o entendimento total;\n",
    "\n",
    "\n",
    "4. Justificativa das conclus√µes obitdas: n√£o existir√° certo ou errado, mas as decis√µes e as conclus√µes precisam ser bem justificadas com base nos resultados obtidos.  \n",
    "\n",
    "O desempenho do modelo **n√£o** ser√° considerado como crit√©rio de avalia√ß√£o.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6049a9c",
   "metadata": {},
   "source": [
    "## Informa√ß√µes gerais\n",
    "\n",
    "- O projeto deve ser desenvolvido individualmente;\n",
    "\n",
    "\n",
    "- Data de divulga√ß√£o: 11/01/2022;\n",
    "\n",
    "\n",
    "- Aula de monitoria: 19/01/2022;\n",
    "\n",
    "\n",
    "- Data de entrega: 26/01/2022;\n",
    "\n",
    "\n",
    "- Entrega atrav√©s do Class: √Årvore de Decis√£o -> Exerc√≠cios -> Projeto 2\n",
    "\n",
    "\n",
    "Anexar, na entrega, o notebook de desenvolvimento e o arquivo .csv de submiss√£o, da seguinte forma:  \n",
    "\n",
    "notebook: ```<nome>_<sobrenome>_<n√∫meroTurma>_projeto_2.ipynb```   \n",
    "csv: ```<nome>_<sobrenome>_<n√∫meroTurma>_projeto_2_submissao.csv```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb23437",
   "metadata": {},
   "source": [
    "## Dicas\n",
    "\n",
    "### Base de treino e submiss√£o\n",
    "\n",
    "A base de submiss√£o n√£o possui a vari√°vel de sa√≠da, portanto ela ser√° utilizada **apenas** para gerar o arquivo que acompanha a submiss√£o do projeto.      \n",
    "\n",
    "### Tente encontrar poss√≠veis vieses\n",
    "\n",
    "√â muito comum que modelos de NLP possuam fortes vieses, como a tend√™ncia de relacionar palavras espec√≠ficas com alguma classe de sa√≠da. Tente encontrar vieses no seu estudo, isso pode ajudar a tirar boas conclus√µes. o campo \"query_used\" pode ser √∫til para essa an√°lise.  \n",
    "\n",
    "### O pr√©-processamento √© a chave para um bom desempenho\n",
    "\n",
    "Essa √© a etapa que mais vai contribuir para o desempenho do seu modelo. Seja criativo e desenvolva essa etapa de uma maneira que seja f√°cil de aplicar o mesmo processamento para uma nova base, voc√™ ter√° que fazer isso para gerar a base de submiss√£o.\n",
    "\n",
    "### Um term√¥metro para o seu desenvolvimento\n",
    "\n",
    "Ap√≥s a corre√ß√£o do seu projeto, o professor ir√° disponibilizar a sua acur√°cia obtida na base de submiss√£o. Voc√™ pode interpretar esse resultado como a simula√ß√£o do resultado do seu modelo em produ√ß√£o. Uma diferen√ßa entre o resultado do estudo e o resultado de submiss√£o indica um grau de **overfitting** no seu modelo.\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96911dea",
   "metadata": {},
   "source": [
    "# Desenvolvimento do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d5233",
   "metadata": {},
   "source": [
    "## 1. An√°lise de consist√™ncia dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f25c3d",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad89b883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas carregadas com sucesso.\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Bibliotecas\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import squarify\n",
    "import plotly.offline as py\n",
    "import plotly_express as px\n",
    "from plotly import graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC \n",
    "# Naive Bayes (Gaussian, Multinomial)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Stochastic Gradient Descent Classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# KNN (k-nearest neighbor)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "# LGBM Classifier\n",
    "from lightgbm import LGBMClassifier\n",
    "# Ada Boosting Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Dummy Boosting Classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from unidecode import unidecode\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#!pip install enelvo\n",
    "from enelvo.normaliser import Normaliser\n",
    "\n",
    "# Abaixo seguem 2 formas para a instala√ß√£o do spaCy: via conda ou pip\n",
    "# Instala√ß√£o utilizando conda\n",
    "#!conda install -c conda-forge spacy\n",
    "\n",
    "# Instala√ß√£o utilizando Pip\n",
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "\n",
    "import spacy\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shap\n",
    "\n",
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from IPython.core.display import HTML as Center\n",
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Bibliotecas carregadas com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1272b9b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Padr√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51314fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Defini√ß√£o de padr√µes para gr√°ficos e cores\n",
    "\n",
    "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
    "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=13)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=13)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=13)    # legend fontsize\n",
    "plt.rc('font', size=13)          # controls default text sizes\n",
    "\n",
    "colors = sns.color_palette(\"pastel\") # deep, pastel, Set1 Set2 Set3, icefire, tab10, muted, colorlind, coolwarm\n",
    "cmap_colors = 'GnBu'\n",
    "\n",
    "font_path = \"./fonts/CabinSketch-Bold.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e73245",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defini√ß√£o de padr√µes para centraliza√ß√£o de gr√°ficos no notebook\n",
    "\n",
    "Center(\"\"\" <style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style> \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9473b925",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Defini√ß√£o de vari√°veis auxiliares\n",
    "\n",
    "rollback = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c2874",
   "metadata": {},
   "source": [
    "### Fun√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3054506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de avalia√ß√£o dos valores de NaN no dataframe\n",
    "\n",
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mz_table = mz_table.rename(\n",
    "        columns = {0 : 'Valores faltantes', 1 : '% de Valores Totais'})\n",
    "        mz_table['Data Type'] = df.dtypes\n",
    "        mz_table = mz_table[\n",
    "            mz_table.iloc[:,1] != 0].sort_values(\n",
    "        '% de Valores Totais', ascending=False).round(1)\n",
    "        print (\"O dataframe tem \" + str(df.shape[1]) + \" colunas e \" + str(df.shape[0]) + \" linhas.\\n\"      \n",
    "            \"Existem \" + str(mz_table.shape[0]) +\n",
    "              \" colunas que t√™m valores faltantes.\")\n",
    "        mz_table.to_excel('missing_and_zero_values.xlsx', freeze_panes=(1,0), index = True)\n",
    "        return mz_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaae7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de avalia√ß√£o de modelos de aprendizagem de m√°quinas\n",
    "\n",
    "def test_models_plot_roc_auc_curve(model_list, col_model_name, col_model, X_train, X_test, y_train, y_test):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    for mdl in model_list:\n",
    "        model = mdl[col_model]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        y_predproba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        #fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predproba)\n",
    "        #auc = metrics.roc_auc_score(y_test, y_predict)\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.accuracy_curve(y_test, y_predproba)        \n",
    "        auc = metrics.accuracy_score(y_test, y_predict)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label='%s ROC (AUC = %0.4f)' % (mdl[col_model_name], auc))\n",
    "        print(\"Model      : %s\" % mdl[col_model_name])\n",
    "        calc_predict(mdl[col_model_name], y_test, y_predict)\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC-AUC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61537f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de avalia√ß√£o de modelos apresenta√ß√£o da matriz de confus√£o\n",
    "\n",
    "def test_models_plot_confusion_matrix(model_list, col_model_name, col_model, X_train, X_test, y_train, y_test):\n",
    "    for mdl in model_list:\n",
    "        model = mdl[col_model]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predict = model.predict(X_test)\n",
    "        \n",
    "        print(\"\")        \n",
    "        print(\"=\" * 55)        \n",
    "        print(\"Model      : %s\" % mdl[col_model_name])\n",
    "        calc_predict(mdl[col_model_name], y_test, y_predict)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_predict)\n",
    "        cm_matrix = pd.DataFrame(data=cm, columns=['Atual Positivo:1', 'Atual Negativo:0'], \n",
    "                                         index=['Pred. Positivo:1', 'Pred. Negativo:0'])\n",
    "\n",
    "        print('Matriz Confus√£o\\n\\n', cm)\n",
    "        print('\\nV. Positivos(VP) = ', cm[0,0])\n",
    "        print('V. Negativos(VN) = ', cm[1,1])\n",
    "        print('F. Positivos(FP) = ', cm[0,1])\n",
    "        print('F. Negativos(FN) = ', cm[1,0])\n",
    "\n",
    "        sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a3ea476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para c√°lcular as m√©tricas \n",
    "\n",
    "def calc_predict(col_model_name, y_test, y_predict):\n",
    "    #print(\"ROC - AUC  : %0.4f \" % metrics.roc_auc_score(y_test, y_predict))\n",
    "    print(\"Accuracy   : %0.4f \" %  accuracy_score(y_test, y_predict))\n",
    "    print(\"Precision  : %0.4f \" % precision_score(y_test, y_predict, average='weighted'))\n",
    "    print(\"Recall     : %0.4f \" % recall_score(y_test, y_predict, average='weighted'))\n",
    "    print(\"F1 - Score : %0.4f \" % f1_score(y_test, y_predict, average='weighted'))\n",
    "    print(\"MAE        : %0.4f \" % mean_absolute_error(y_test, y_predict))\n",
    "    print(\"RMSE       : %0.4f \" % np.sqrt(mean_squared_error(y_test, y_predict)))\n",
    "    print(\"R2         : %0.4f \" % r2_score(y_test, y_predict))\n",
    "    print(\"\")\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print(\"=\" * 55)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f11adbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para calcular a import√¢ncia da vari√°vel no modelo\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "def modelfit(alg, dtrain, predictors, target, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    # Adequando as classes para treino\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "        \n",
    "    # Previs√£o de sa√≠da para o conjunto de dados de teste\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    # Utilizando o Cross Validation\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, dtrain[predictors], dtrain[target], cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Exibindo relat√≥rio:\n",
    "    print (f\"\\nRelat√≥rio do Modelo {alg}\")\n",
    "    print (\"\\nAcuracia : %.4g\" % metrics.accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    #print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain[target], dtrain_predictions))\n",
    "    \n",
    "    if performCV:\n",
    "        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "        \n",
    "    #Exibindo gr√°fico da importancia das vari√°veis\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Import√¢ncia das vari√°veis', color=colors)\n",
    "        plt.ylabel('Pontua√ß√£o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52180e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para gera√ß√£o de cores\n",
    "\n",
    "def random_colours(number_of_colors):\n",
    "    '''\n",
    "    Fun√ß√£o simples para gera√ß√£o de cores aleat√≥rias.\n",
    "    Entrada:\n",
    "        n√∫mero_de_cores - valor inteiro indicando o n√∫mero de cores que v√£o ser geradas.\n",
    "    Sa√≠da:\n",
    "        Cor no seguinte formato: ['#E86DA4'].\n",
    "        \n",
    "    '''\n",
    "    colors = []\n",
    "    for i in range(number_of_colors):\n",
    "        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n",
    "    return colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2553f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para contagem de palavras \n",
    "\n",
    "def words_unique(sentiment, numwords, raw_words):\n",
    "    '''\n",
    "    Entrada:\n",
    "        segmento - Categoria do segmento (ex. 2 = 'Neutro');\n",
    "        numwords - quantas palavras espec√≠ficas se pretende ver no resultado final; \n",
    "        raw_words - lista do texto;\n",
    "        \n",
    "    Resultado: \n",
    "        dataframe com informa√ß√£o sobre a palavra espec√≠fica e quantas vezes aparece no texto (por ordem decrescente com base nas suas contagens).\n",
    "\n",
    "    '''\n",
    "    allother = []\n",
    "    for item in dfc[dfc.sentiment != sentiment]['list_words']:\n",
    "        for word in item:\n",
    "            allother .append(word)\n",
    "    allother  = list(set(allother ))\n",
    "    \n",
    "    specificnonly = [x for x in raw_text if x not in allother]\n",
    "    \n",
    "    mycounter = Counter()\n",
    "    \n",
    "    for item in dfc[dfc.sentiment == sentiment]['list_words']:\n",
    "        for word in item:\n",
    "            mycounter[word] += 1\n",
    "    keep = list(specificnonly)\n",
    "    \n",
    "    for word in list(mycounter):\n",
    "        if word not in keep:\n",
    "            del mycounter[word]\n",
    "    \n",
    "    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n",
    "    \n",
    "    return Unique_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "922f869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\johnny.horita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnny.horita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Abaixo seguem 2 formas para a instala√ß√£o do spaCy: via conda ou pip\n",
    "\n",
    "# Instala√ß√£o utilizando conda\n",
    "#!conda install -c conda-forge spacy\n",
    "\n",
    "# Instala√ß√£o utilizando Pip\n",
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "\n",
    "# Bilbioteca em portugues\n",
    "# Efficiency\n",
    "#!python -m spacy download pt_core_news_sm\n",
    "\n",
    "# Accuracy\n",
    "#!python -m spacy download pt_core_news_lg\n",
    "\n",
    "spc_pt = spacy.load('pt_core_news_lg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "#Adicionando stopwords que n√£o est√£o na lista do nltk \n",
    "stopwords.append(\"'\")\n",
    "stopwords.append(\"pra\")\n",
    "stopwords.append(\"t√°\")\n",
    "stopwords.append(\"t√£o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad860da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para tratamento da vari√°vel de texto para definir melhores valores para classifica√ß√£o da modelagem\n",
    "# e ou normalizar o tratamento da vari√°vel de texto utilizando a biblioteca enelvo\n",
    "\n",
    "# instanciando\n",
    "normalizador = Normaliser(tokenizer='readable', sanitize=True, capitalize_acs=True, capitalize_pns=True) # capitalize_inis=True\n",
    "\n",
    "def nlp_tratar_texto(texto, normalize=False):\n",
    "    #Remover endere√ßos de sites\n",
    "    texto_sem_url = re.sub(r'https?:\\/\\/\\S+', '', texto)\n",
    "        \n",
    "    #Remover e-mail / users\n",
    "    #texto_sem_email = re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*\\.?[A-Za-z0-9]*', '', texto_sem_url)\n",
    "    texto_sem_email = re.sub(r'[A-Za-z0-9]*@\\S+', '', texto_sem_url)\n",
    "    \n",
    "    if normalize==True:\n",
    "        # Tratamento do texto utilizando o enelvo (Normalize)\n",
    "        texto_norm = normalizador.normalise(texto_sem_email)\n",
    "    \n",
    "        #Remover caracteres que n√£o s√£o letras e tokeniza√ß√£o\n",
    "        texto_tratado =  re.findall(r'\\b[A-z√Ä-√∫√º]+\\b', texto_norm.lower())\n",
    "    else:\n",
    "        texto_tratado =  re.findall(r'\\b[A-z√Ä-√∫√º]+\\b', texto_sem_email.lower())\n",
    "\n",
    "    #Remover stopwords\n",
    "    stop = set(stopwords)\n",
    "    palavras = [w for w in texto_tratado if w not in stop]\n",
    "    palavras_string = \" \".join(palavras)\n",
    "\n",
    "    #Instanciar o objeto spacy\n",
    "    spc_letras =  spc_pt(palavras_string)\n",
    "\n",
    "    #Lemmiza√ß√£o \n",
    "    tokens = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "\n",
    "    #problemas com verbo ir\n",
    "    ir = ['vou', 'vais', 'vai', 'vamos', 'ides', 'v√£o']\n",
    "    tokens = ['ir' if token in ir else str(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a733446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para apresentar nuvem de palavras\n",
    "\n",
    "def plot_wordcloud(text, title = None, backcolor = 'white', clrmap = ''):\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "                            background_color=backcolor,\n",
    "                            width=2000, \n",
    "                            height=800,\n",
    "                            colormap=clrmap, \n",
    "                            font_path=font_path, \n",
    "                            collocations = False)\n",
    "\n",
    "    wordcloud.generate(text)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c39e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e738a2b7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Inicializando Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39c224f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Importando arquivo\n",
    "\n",
    "df = pd.read_csv('./dados/train/Train3Classes.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd96274",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas...........: 95000\n",
      "Quantidade de linhas duplicadas: 0\n",
      "Quantidade de colunas..........: 5\n"
     ]
    }
   ],
   "source": [
    "# Quantidade de linhas e colunas\n",
    "\n",
    "qtl, qtc = df.shape\n",
    "\n",
    "# Quantidade de linhas duplicadas\n",
    "\n",
    "qtd, _ = df[df.duplicated(keep=False)].shape\n",
    "\n",
    "print(f'Quantidade de linhas...........: {qtl}')\n",
    "print(f'Quantidade de linhas duplicadas: {qtd}')\n",
    "print(f'Quantidade de colunas..........: {qtc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf0b35b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95000 entries, 0 to 94999\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          95000 non-null  int64 \n",
      " 1   tweet_text  95000 non-null  object\n",
      " 2   tweet_date  95000 non-null  object\n",
      " 3   sentiment   95000 non-null  int64 \n",
      " 4   query_used  95000 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Informa√ß√µes do dataframe\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95aba7f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataframe tem 5 colunas e 95000 linhas.\n",
      "Existem 0 colunas que t√™m valores faltantes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valores faltantes</th>\n",
       "      <th>% de Valores Totais</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Valores faltantes, % de Valores Totais, Data Type]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando os valores nulos do dataframe\n",
    "\n",
    "missing_values_table(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abf0cc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Conclus√µes:** \n",
    "\n",
    "O dataframe √© composto por 05 colunas e 95.000 registros.\n",
    "\n",
    "A tabela acima N√ÉO apresenta valores faltantes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c26c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc5d77c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tweet_text', 'tweet_date', 'sentiment', 'query_used'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de colunas do dataframe\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c9ae92d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1049721159292346368</td>\n",
       "      <td>Rio elege maior bancada policial de sua hist√≥r...</td>\n",
       "      <td>Tue Oct 09 18:00:01 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>folha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1046251157025423360</td>\n",
       "      <td>fiquei t√£o triste quando eu vi o pre√ßo da c√¢me...</td>\n",
       "      <td>Sun Sep 30 04:11:28 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1041744620206653440</td>\n",
       "      <td>Para Theresa May, seu plano para o Brexit √© a ...</td>\n",
       "      <td>Mon Sep 17 17:44:06 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>exame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1046937084727107589</td>\n",
       "      <td>caralho eu quero proteger a danielly em um pot...</td>\n",
       "      <td>Tue Oct 02 01:37:06 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1047326854229778432</td>\n",
       "      <td>@SiCaetano_ viva o caos :)</td>\n",
       "      <td>Wed Oct 03 03:25:55 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         tweet_text  \\\n",
       "0  1049721159292346368  Rio elege maior bancada policial de sua hist√≥r...   \n",
       "1  1046251157025423360  fiquei t√£o triste quando eu vi o pre√ßo da c√¢me...   \n",
       "2  1041744620206653440  Para Theresa May, seu plano para o Brexit √© a ...   \n",
       "3  1046937084727107589  caralho eu quero proteger a danielly em um pot...   \n",
       "4  1047326854229778432                         @SiCaetano_ viva o caos :)   \n",
       "\n",
       "                       tweet_date  sentiment query_used  \n",
       "0  Tue Oct 09 18:00:01 +0000 2018          2      folha  \n",
       "1  Sun Sep 30 04:11:28 +0000 2018          0         :(  \n",
       "2  Mon Sep 17 17:44:06 +0000 2018          2      exame  \n",
       "3  Tue Oct 02 01:37:06 +0000 2018          0         :(  \n",
       "4  Wed Oct 03 03:25:55 +0000 2018          1         :)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listagem das primeiras linhas do dataframe\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5fbcb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5811661",
   "metadata": {},
   "source": [
    "## 2. Pr√©-processamento e transforma√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a59eb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Tratamento de vari√°veis**\n",
    "\n",
    "- Cria√ß√£o da vari√°vel **filtered_words**, onde o texto original (tweet_text) ser√° submetido a tratamentos para definir as palavras chaves de sentimento para melhorar a classifica√ß√£o do modelo;<br>\n",
    "\n",
    "\n",
    "- Cria√ß√£o da vari√°vel **join_f_words**, para concatenar as palavras do coluna **filtered_words**;<br>\n",
    "\n",
    "\n",
    "- Cria√ß√£o da vari√°vel **normalize_words**, para normalizar a coluna **join_f_words** utilizando a biblioteca **enelvo**;<br>\n",
    "\n",
    "\n",
    "- Exclus√£o de vari√°veis que entendemos n√£o ser relevante para o modelo<br>\n",
    "    **id**: ID √∫nico para o tweet<br>\n",
    "    **tweet_date**: Data da publica√ß√£o no Twitter<br>\n",
    "    **query_used**: Filtro utilizado para buscar a publica√ß√£o<br>\n",
    "\n",
    "\n",
    "- Exclus√£o de vari√°veis tratadas<br>\n",
    "    **tweet_text**: Texto da publica√ß√£o no Twitter<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d247977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# O tratamendo dos tweets leva em m√©dia 10 min, em uma m√°quina i7 com 12 cores e 32 ram \n",
    "\n",
    "# Cria uma nova vari√°vel utilizando fun√ß√µes para tratamento das palavras do texto\n",
    "df['filtered_words'] = df['tweet_text'].apply(lambda w: nlp_tratar_texto(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0afad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma nova vari√°vel concatenando os tokens gerados pelo tratamento das palavras gerando um novo texto reduzido\n",
    "\n",
    "df['join_f_words'] = df['filtered_words'].apply(lambda w: ' '.join(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3effc2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# O tratamendo dos tweets utilizando o biblioteca enelvo leva em m√©dia 4 horas, em uma m√°quina i7 com 12 cores e 32 ram \n",
    "\n",
    "# Cria uma nova vari√°vel normalizando os tokens do novo texto reduzido\n",
    "\n",
    "#df['normalize_words'] = df['join_f_words'].apply(lambda w: nlp_normalizar_texto(w))\n",
    "#df['normalize_words'] = df['tweet_text'].apply(lambda w: nlp_tratar_texto(w, normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a43d679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>filtered_words</th>\n",
       "      <th>join_f_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rio elege maior bancada policial de sua hist√≥ria https://t.co/sGXnhZKrHx https://t.co/Mcgiz70jPF</td>\n",
       "      <td>[rio, eleger, maior, bancada, policial, hist√≥ria]</td>\n",
       "      <td>rio eleger maior bancada policial hist√≥ria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fiquei t√£o triste quando eu vi o pre√ßo da c√¢mera :((((</td>\n",
       "      <td>[ficar, triste, vir, pre√ßo, c√¢mera]</td>\n",
       "      <td>ficar triste vir pre√ßo c√¢mera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Para Theresa May, seu plano para o Brexit √© a √∫nica op√ß√£o https://t.co/epl39YD9bj</td>\n",
       "      <td>[theresa, may, plano, brexit, √∫nica, op√ß√£o]</td>\n",
       "      <td>theresa may plano brexit √∫nica op√ß√£o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>caralho eu quero proteger a danielly em um pote tadinhaa :(</td>\n",
       "      <td>[caralho, querer, proteger, danielly, pote, tadinhaa]</td>\n",
       "      <td>caralho querer proteger danielly pote tadinhaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@SiCaetano_ viva o caos :)</td>\n",
       "      <td>[vivo, caos]</td>\n",
       "      <td>vivo caos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94995</th>\n",
       "      <td>Cuba e defensor de direitos humanos se unem contra chefe da OEA, interven√ß√£o militar na Venezuela. https://t.co/Lo2RvIgFBA https://t.co/CiPddCuRvw</td>\n",
       "      <td>[cuba, defensor, direitos, humanos, unir, contra, chefe, oea, interven√ß√£o, militar, venezuela]</td>\n",
       "      <td>cuba defensor direitos humanos unir contra chefe oea interven√ß√£o militar venezuela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94996</th>\n",
       "      <td>#Oportunidade ‚û°Ô∏è Venha fazer parte da nossa equipe! Vagas abertas para alunos de Administra√ß√£o. Envie seu curr√≠culo e comprovante de matr√≠cula para an√°lise pr√©via at√© √†s 12h do dia 24/08, para o e-mail: vagasestagio@sistemafieto.com.br üòâ https://t.co/qvdFYeJh9I</td>\n",
       "      <td>[oportunidade, vir, fazer, parte, equipe, vagas, aberto, alunos, administra√ß√£o, enviar, curr√≠culo, comprovante, matr√≠cula, an√°lise, pr√©via, dia, mail]</td>\n",
       "      <td>oportunidade vir fazer parte equipe vagas aberto alunos administra√ß√£o enviar curr√≠culo comprovante matr√≠cula an√°lise pr√©via dia mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94997</th>\n",
       "      <td>@96syoo EU SEI üò≠üò≠ √© por isso que significa muito!! To feliz demais, eu amo ela :( e aqui da pra ver que ela deixa a bandeira na frente do palco e sai correndo pra pegar garrafa de √°gua mas depois disso ela pegou de novo üòÇ https://t.co/82oPAXYVNC</td>\n",
       "      <td>[saber, significar, to, feliz, demais, amar, aqui, ver, deixar, bandeira, frente, palco, sair, correr, pegar, garrafa, √°gua, disso, pegar, novo]</td>\n",
       "      <td>saber significar to feliz demais amar aqui ver deixar bandeira frente palco sair correr pegar garrafa √°gua disso pegar novo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94998</th>\n",
       "      <td>@louistsexhes N te conhe√ßo mas posta :D</td>\n",
       "      <td>[n, conhecer, posto, d]</td>\n",
       "      <td>n conhecer posto d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94999</th>\n",
       "      <td>meu deus :( https://t.co/BlXazxZeKq</td>\n",
       "      <td>[deus]</td>\n",
       "      <td>deus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                  tweet_text  \\\n",
       "0      Rio elege maior bancada policial de sua hist√≥ria https://t.co/sGXnhZKrHx https://t.co/Mcgiz70jPF                                                                                                                                                                        \n",
       "1      fiquei t√£o triste quando eu vi o pre√ßo da c√¢mera :((((                                                                                                                                                                                                                  \n",
       "2      Para Theresa May, seu plano para o Brexit √© a √∫nica op√ß√£o https://t.co/epl39YD9bj                                                                                                                                                                                       \n",
       "3      caralho eu quero proteger a danielly em um pote tadinhaa :(                                                                                                                                                                                                             \n",
       "4      @SiCaetano_ viva o caos :)                                                                                                                                                                                                                                              \n",
       "...                           ...                                                                                                                                                                                                                                              \n",
       "94995  Cuba e defensor de direitos humanos se unem contra chefe da OEA, interven√ß√£o militar na Venezuela. https://t.co/Lo2RvIgFBA https://t.co/CiPddCuRvw                                                                                                                      \n",
       "94996  #Oportunidade ‚û°Ô∏è Venha fazer parte da nossa equipe! Vagas abertas para alunos de Administra√ß√£o. Envie seu curr√≠culo e comprovante de matr√≠cula para an√°lise pr√©via at√© √†s 12h do dia 24/08, para o e-mail: vagasestagio@sistemafieto.com.br üòâ https://t.co/qvdFYeJh9I   \n",
       "94997  @96syoo EU SEI üò≠üò≠ √© por isso que significa muito!! To feliz demais, eu amo ela :( e aqui da pra ver que ela deixa a bandeira na frente do palco e sai correndo pra pegar garrafa de √°gua mas depois disso ela pegou de novo üòÇ https://t.co/82oPAXYVNC                   \n",
       "94998  @louistsexhes N te conhe√ßo mas posta :D                                                                                                                                                                                                                                 \n",
       "94999  meu deus :( https://t.co/BlXazxZeKq                                                                                                                                                                                                                                     \n",
       "\n",
       "                                                                                                                                               filtered_words  \\\n",
       "0      [rio, eleger, maior, bancada, policial, hist√≥ria]                                                                                                        \n",
       "1      [ficar, triste, vir, pre√ßo, c√¢mera]                                                                                                                      \n",
       "2      [theresa, may, plano, brexit, √∫nica, op√ß√£o]                                                                                                              \n",
       "3      [caralho, querer, proteger, danielly, pote, tadinhaa]                                                                                                    \n",
       "4      [vivo, caos]                                                                                                                                             \n",
       "...             ...                                                                                                                                             \n",
       "94995  [cuba, defensor, direitos, humanos, unir, contra, chefe, oea, interven√ß√£o, militar, venezuela]                                                           \n",
       "94996  [oportunidade, vir, fazer, parte, equipe, vagas, aberto, alunos, administra√ß√£o, enviar, curr√≠culo, comprovante, matr√≠cula, an√°lise, pr√©via, dia, mail]   \n",
       "94997  [saber, significar, to, feliz, demais, amar, aqui, ver, deixar, bandeira, frente, palco, sair, correr, pegar, garrafa, √°gua, disso, pegar, novo]         \n",
       "94998  [n, conhecer, posto, d]                                                                                                                                  \n",
       "94999  [deus]                                                                                                                                                   \n",
       "\n",
       "                                                                                                                               join_f_words  \n",
       "0      rio eleger maior bancada policial hist√≥ria                                                                                            \n",
       "1      ficar triste vir pre√ßo c√¢mera                                                                                                         \n",
       "2      theresa may plano brexit √∫nica op√ß√£o                                                                                                  \n",
       "3      caralho querer proteger danielly pote tadinhaa                                                                                        \n",
       "4      vivo caos                                                                                                                             \n",
       "...          ...                                                                                                                             \n",
       "94995  cuba defensor direitos humanos unir contra chefe oea interven√ß√£o militar venezuela                                                    \n",
       "94996  oportunidade vir fazer parte equipe vagas aberto alunos administra√ß√£o enviar curr√≠culo comprovante matr√≠cula an√°lise pr√©via dia mail  \n",
       "94997  saber significar to feliz demais amar aqui ver deixar bandeira frente palco sair correr pegar garrafa √°gua disso pegar novo           \n",
       "94998  n conhecer posto d                                                                                                                    \n",
       "94999  deus                                                                                                                                  \n",
       "\n",
       "[95000 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Altera o tamanho da visualiza√ß√£o das colunas para demonstrar todo o conte√∫do\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "\n",
    "# Mostra o conte√∫do das vari√°veis tratadas\n",
    "#df[['tweet_text', 'filtered_words', 'join_f_words', 'normalize_words']]\n",
    "df[['tweet_text', 'filtered_words', 'join_f_words']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e0f72",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Conclus√µes**\n",
    "\n",
    "Submetemos a vari√°vel tweet_text ao processamento de texto:\n",
    "\n",
    "- Remo√ß√£o de endere√ßos de sites;\n",
    "- Remo√ß√£o de e-mails e usu√°rios do tweet;\n",
    "- Remo√ß√£o de caracteres que n√£o s√£o letras;\n",
    "- Remo√ß√£o de d√≠gitos;\n",
    "- Transforma√ß√£o de todas as palavras para min√∫sculas;\n",
    "- Tokeniza√ß√£o do texto;\n",
    "- Remo√ß√£o de stopwords (portugues);\n",
    "- \"Lemmatiza√ß√£o\" do texto;\n",
    "\n",
    "Ap√≥s o tratamento do texto foram criadas duas vari√°veis para gravar as informa√ß√µes do texto tratado, entendemos que estas vari√°veis estejam mais limpas para utiliza√ß√£o na modelagem.\n",
    "\n",
    "- **join_f_words** que contem o texto tratado com o processamento descrito acima;\n",
    "- **normalize_words** que cont√©m o texto tratado e a normaliza√ß√£o de texto utilizando a biblioteca **enelvo** que \"corrige\" abrevia√ß√µes, g√≠rias, erros ortogr√°ficos e acr√¥nimos;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597bb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a232e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame gravado com successo.\n"
     ]
    }
   ],
   "source": [
    "#Grava um novo CSV com as novas colunas para avalia√ß√£o\n",
    "\n",
    "# Definindo o nome do arquivo\n",
    "file_name = './dados/train/Train3Classes_with_Tokens.csv'\n",
    "  \n",
    "# Salvando o CSV\n",
    "try:\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print('DataFrame gravado com successo.')\n",
    "except:\n",
    "    print(\"Ocorreu um erro na grava√ß√£o.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51a02c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo de rollback do dataframe n√£o executado.\n"
     ]
    }
   ],
   "source": [
    "# Para executar esta celula descomentar o comando **#rollback = 1** para restaurar o df ap√≥s tratamento de texto,\n",
    "# para testes de repeti√ß√£o, devido ao tempo de normaliza√ß√£o do texto\n",
    "\n",
    "#rollback = 1\n",
    "\n",
    "# Restaurar o dataframe df\n",
    "if rollback == 1:\n",
    "    # Carregando csv\n",
    "    try:\n",
    "        df = pd.read_csv('./dados/train/Train3Classes_with_Tokens.csv')\n",
    "        \n",
    "        DeleteList=['Unnamed: 0']\n",
    "        df = df.drop(DeleteList, axis=1).copy()\n",
    "        \n",
    "        print('Arquivo carregado com successo.')\n",
    "        rollback = 0\n",
    "    except:\n",
    "        print('Ocorreu um erro no carregamento do arquivo.')\n",
    "else:\n",
    "    print('Processo de rollback do dataframe n√£o executado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46761af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query_used</th>\n",
       "      <th>filtered_words</th>\n",
       "      <th>join_f_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1046982734827200513</td>\n",
       "      <td>N√£o mais :(( https://t.co/3omrxaGTJc</td>\n",
       "      <td>Tue Oct 02 04:38:30 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>1049147450613792768</td>\n",
       "      <td>@yoongxsmin Eu tamb√©m :(</td>\n",
       "      <td>Mon Oct 08 04:00:19 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>1046999889597616133</td>\n",
       "      <td>@brooklenbarbie eu tamb√©m :(</td>\n",
       "      <td>Tue Oct 02 05:46:40 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1046276432241004544</td>\n",
       "      <td>@ywkheiz 1,57 s√≥ :( e a sua?</td>\n",
       "      <td>Sun Sep 30 05:51:54 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1047533065252327425</td>\n",
       "      <td>@b1ktwpas E √© mesmo! : )</td>\n",
       "      <td>Wed Oct 03 17:05:19 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93081</th>\n",
       "      <td>1047538834462969856</td>\n",
       "      <td>https://t.co/iHSV24IOgV</td>\n",
       "      <td>Wed Oct 03 17:28:15 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>veja</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93294</th>\n",
       "      <td>1049253272220061696</td>\n",
       "      <td>@charroeciencia Que foi :(</td>\n",
       "      <td>Mon Oct 08 11:00:48 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94127</th>\n",
       "      <td>1049296782298173441</td>\n",
       "      <td>@JoaoGSimoes A minha e a tua :))</td>\n",
       "      <td>Mon Oct 08 13:53:42 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94206</th>\n",
       "      <td>1045413174181343232</td>\n",
       "      <td>e foi :)</td>\n",
       "      <td>Thu Sep 27 20:41:38 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>:)</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94573</th>\n",
       "      <td>1046256494222725120</td>\n",
       "      <td>@antunesaranda muito :(</td>\n",
       "      <td>Sun Sep 30 04:32:41 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>:(</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                            tweet_text  \\\n",
       "215    1046982734827200513  N√£o mais :(( https://t.co/3omrxaGTJc   \n",
       "286    1049147450613792768  @yoongxsmin Eu tamb√©m :(               \n",
       "476    1046999889597616133  @brooklenbarbie eu tamb√©m :(           \n",
       "592    1046276432241004544  @ywkheiz 1,57 s√≥ :( e a sua?           \n",
       "862    1047533065252327425  @b1ktwpas E √© mesmo! : )               \n",
       "...                    ...                       ...               \n",
       "93081  1047538834462969856  https://t.co/iHSV24IOgV                \n",
       "93294  1049253272220061696  @charroeciencia Que foi :(             \n",
       "94127  1049296782298173441  @JoaoGSimoes A minha e a tua :))       \n",
       "94206  1045413174181343232  e foi :)                               \n",
       "94573  1046256494222725120  @antunesaranda muito :(                \n",
       "\n",
       "                           tweet_date  sentiment query_used filtered_words  \\\n",
       "215    Tue Oct 02 04:38:30 +0000 2018  0          :(         []              \n",
       "286    Mon Oct 08 04:00:19 +0000 2018  0          :(         []              \n",
       "476    Tue Oct 02 05:46:40 +0000 2018  0          :(         []              \n",
       "592    Sun Sep 30 05:51:54 +0000 2018  0          :(         []              \n",
       "862    Wed Oct 03 17:05:19 +0000 2018  1          :)         []              \n",
       "...                               ... ..          ..         ..              \n",
       "93081  Wed Oct 03 17:28:15 +0000 2018  2          veja       []              \n",
       "93294  Mon Oct 08 11:00:48 +0000 2018  0          :(         []              \n",
       "94127  Mon Oct 08 13:53:42 +0000 2018  1          :)         []              \n",
       "94206  Thu Sep 27 20:41:38 +0000 2018  1          :)         []              \n",
       "94573  Sun Sep 30 04:32:41 +0000 2018  0          :(         []              \n",
       "\n",
       "      join_f_words  \n",
       "215                 \n",
       "286                 \n",
       "476                 \n",
       "592                 \n",
       "862                 \n",
       "...   ..            \n",
       "93081               \n",
       "93294               \n",
       "94127               \n",
       "94206               \n",
       "94573               \n",
       "\n",
       "[364 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando quais linhas n√£o apresentam valor ap√≥s o tratamento do texto\n",
    "\n",
    "df[np.where((df['join_f_words'].str.len()<1), True, False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d610ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando novo dataframe retirando as linhas que ficaram sem texto\n",
    "\n",
    "# Novo dataframe dfc (df copy)\n",
    "dfc = df[np.where((df['join_f_words'].str.len()>1), True, False)].copy()\n",
    "\n",
    "# Reindexando os indices do dataframe\n",
    "dfc.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "327e65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisar o momento dessa execu√ß√£o\n",
    "\n",
    "# Exclus√£o de vari√°veis que entendemos n√£o ser relevante para o modelo\n",
    "\n",
    "# id: ID √∫nico para o tweet  \n",
    "# tweet_text: Texto da publica√ß√£o no Twitter  \n",
    "# tweet_date: Data da publica√ß√£o no Twitter  \n",
    "# query_used: Filtro utilizado para buscar a publica√ß√£o\n",
    "# filtered_words: Texto da publica√ß√£o no Twitter ap√≥s tratamento de texto\n",
    "\n",
    "# Excluindo colunas\n",
    "#DeleteList=['id', 'tweet_text', 'tweet_date', 'query_used', 'filtered_words']\n",
    "DeleteList=['id', 'tweet_date', 'query_used', 'filtered_words']\n",
    "\n",
    "# Novo dataframe dfc (df copy)\n",
    "dfc = dfc.drop(DeleteList, axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37f1be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94604 entries, 0 to 94603\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   tweet_text    94604 non-null  object\n",
      " 1   sentiment     94604 non-null  int64 \n",
      " 2   join_f_words  94604 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Informa√ß√µes do dataframe\n",
    "\n",
    "dfc.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50e7cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataframe tem 3 colunas e 94604 linhas.\n",
      "Existem 0 colunas que t√™m valores faltantes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valores faltantes</th>\n",
       "      <th>% de Valores Totais</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Valores faltantes, % de Valores Totais, Data Type]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando os valores nulos do dataframe\n",
    "\n",
    "missing_values_table(dfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2305832",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Conclus√µes:** \n",
    "\n",
    "O dataframe √© composto por 5 colunas e 94.604 registros.\n",
    "\n",
    "A tabela acima apresenta valores faltantes, por√©m como n√£o representa um percentual considerav√©l optamos em excluir os dados faltantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a785713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclus√£o dos registros faltantes\n",
    "\n",
    "dfc.dropna() \n",
    "dfc.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2887d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe original - df\n",
      "Quantidade de linhas...........: 95000\n",
      "Quantidade de colunas..........: 5\n",
      "\n",
      "Dataframe tratado - dfc\n",
      "Quantidade de linhas...........: 94604\n",
      "Quantidade de linhas duplicadas: 811\n",
      "Quantidade de colunas..........: 3\n",
      "\n",
      "Percentual de registros retirados: 0.42%\n"
     ]
    }
   ],
   "source": [
    "# Quantidade de linhas e colunas\n",
    "\n",
    "qtlc, qtcc = dfc.shape\n",
    "\n",
    "# Quantidade de linhas duplicadas\n",
    "qtd, _ = dfc[dfc.duplicated()].shape\n",
    "\n",
    "\n",
    "print('Dataframe original - df')\n",
    "print(f'Quantidade de linhas...........: {qtl}')\n",
    "print(f'Quantidade de colunas..........: {qtc}')\n",
    "\n",
    "print('\\nDataframe tratado - dfc')\n",
    "print(f'Quantidade de linhas...........: {qtlc}')\n",
    "print(f'Quantidade de linhas duplicadas: {qtd}')\n",
    "print(f'Quantidade de colunas..........: {qtcc}')\n",
    "\n",
    "print (f'\\nPercentual de registros retirados: {(100  * (qtl-qtlc) / qtl):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3416ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>join_f_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74022</th>\n",
       "      <td>\"Em recupera√ß√£o judicial, Eternit v√™ receita crescer em agosto\" https://t.co/aKl26jNJyJ #trabalho #feedly</td>\n",
       "      <td>2</td>\n",
       "      <td>recupera√ß√£o judicial eternit ver receita crescer agosto trabalho feedly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36418</th>\n",
       "      <td>#CulturalSpam serale... #notizie #curiose, #interessanti, talvolta #utili! :D #curious, #interesting, sometimes #useful #news! :D Des #nouvelles #curieuses, #int√©ressantes, parfois #utiles! https://t.co/MjC5j2svqe :D</td>\n",
       "      <td>1</td>\n",
       "      <td>culturalspam serale notizie curiose interessanti talvolta utili d curious interesting sometimes useful news d des nouvelles curieuses int√©ressantes parfois utiles d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66554</th>\n",
       "      <td>#FollowFriday Todos os meus seguidores. :) #ff</td>\n",
       "      <td>1</td>\n",
       "      <td>followfriday todos seguidores ff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30159</th>\n",
       "      <td>#FollowFriday para todos meus queridos seguidores. :) #IFTTT</td>\n",
       "      <td>1</td>\n",
       "      <td>followfriday todos queridos seguidores ifttt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>#Novidade Novo Kindle Oasis, tela de 7‚Äù sens√≠vel ao toque de alta resolu√ß√£o, √† prova d‚Äô√°gua, ilumina√ß√£o embutida, Wi-Fi https://t.co/kLVZK6xkBp Confira o pre√ßo no site. #kindle #amazon #leitor #ebook #KindleUnlimited #kindledeals #kindlebook</td>\n",
       "      <td>2</td>\n",
       "      <td>novidade novo kindle oasis tela sens√≠vel toque alta resolu√ß√£o prova d √°gua ilumina√ß√£o embutir wi fi conferir pre√ßo site kindle amazon leitor ebook kindleunlimited kindledeals kindlebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39077</th>\n",
       "      <td>t√©dio :(</td>\n",
       "      <td>0</td>\n",
       "      <td>t√©dio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55345</th>\n",
       "      <td>us2 q uminom :(((</td>\n",
       "      <td>0</td>\n",
       "      <td>q uminom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39161</th>\n",
       "      <td>voltei :)</td>\n",
       "      <td>1</td>\n",
       "      <td>voltar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88831</th>\n",
       "      <td>vou tentar dormir :(</td>\n",
       "      <td>0</td>\n",
       "      <td>ir tentar dormir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60822</th>\n",
       "      <td>üò±NOVOS FILMES E S√âRIES EST√ÉO CHEGANDO NA NETFLIX ESTA SEMANAüëâhttps://t.co/pKE3MfHuwN #netflix #netflixbrasil #filmes #s√©ries #anima√ß√£o #desenho #anime #document√°rio #movie #cinema #sdv #bolsonaro #net #youtube #not√≠cia #news #assistir #baixar #lan√ßamentos #estreias #novidades</td>\n",
       "      <td>2</td>\n",
       "      <td>novos filmes s√©ries chegar netflix semana netflix netflixbrasil filmes s√©ries anima√ß√£o desenho anime document√°rio movie cinema sdv bolsonaro net youtube not√≠cia news assistir baixar lan√ßamentos estreias novidades</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>811 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                 tweet_text  \\\n",
       "74022  \"Em recupera√ß√£o judicial, Eternit v√™ receita crescer em agosto\" https://t.co/aKl26jNJyJ #trabalho #feedly                                                                                                                                                                              \n",
       "36418  #CulturalSpam serale... #notizie #curiose, #interessanti, talvolta #utili! :D #curious, #interesting, sometimes #useful #news! :D Des #nouvelles #curieuses, #int√©ressantes, parfois #utiles! https://t.co/MjC5j2svqe :D                                                               \n",
       "66554  #FollowFriday Todos os meus seguidores. :) #ff                                                                                                                                                                                                                                         \n",
       "30159  #FollowFriday para todos meus queridos seguidores. :) #IFTTT                                                                                                                                                                                                                           \n",
       "24782  #Novidade Novo Kindle Oasis, tela de 7‚Äù sens√≠vel ao toque de alta resolu√ß√£o, √† prova d‚Äô√°gua, ilumina√ß√£o embutida, Wi-Fi https://t.co/kLVZK6xkBp Confira o pre√ßo no site. #kindle #amazon #leitor #ebook #KindleUnlimited #kindledeals #kindlebook                                      \n",
       "...                                                                                                                                                                                                                                                  ...                                      \n",
       "39077  t√©dio :(                                                                                                                                                                                                                                                                               \n",
       "55345  us2 q uminom :(((                                                                                                                                                                                                                                                                      \n",
       "39161  voltei :)                                                                                                                                                                                                                                                                              \n",
       "88831  vou tentar dormir :(                                                                                                                                                                                                                                                                   \n",
       "60822  üò±NOVOS FILMES E S√âRIES EST√ÉO CHEGANDO NA NETFLIX ESTA SEMANAüëâhttps://t.co/pKE3MfHuwN #netflix #netflixbrasil #filmes #s√©ries #anima√ß√£o #desenho #anime #document√°rio #movie #cinema #sdv #bolsonaro #net #youtube #not√≠cia #news #assistir #baixar #lan√ßamentos #estreias #novidades   \n",
       "\n",
       "       sentiment  \\\n",
       "74022  2           \n",
       "36418  1           \n",
       "66554  1           \n",
       "30159  1           \n",
       "24782  2           \n",
       "...   ..           \n",
       "39077  0           \n",
       "55345  0           \n",
       "39161  1           \n",
       "88831  0           \n",
       "60822  2           \n",
       "\n",
       "                                                                                                                                                                                                               join_f_words  \n",
       "74022  recupera√ß√£o judicial eternit ver receita crescer agosto trabalho feedly                                                                                                                                               \n",
       "36418  culturalspam serale notizie curiose interessanti talvolta utili d curious interesting sometimes useful news d des nouvelles curieuses int√©ressantes parfois utiles d                                                  \n",
       "66554  followfriday todos seguidores ff                                                                                                                                                                                      \n",
       "30159  followfriday todos queridos seguidores ifttt                                                                                                                                                                          \n",
       "24782  novidade novo kindle oasis tela sens√≠vel toque alta resolu√ß√£o prova d √°gua ilumina√ß√£o embutir wi fi conferir pre√ßo site kindle amazon leitor ebook kindleunlimited kindledeals kindlebook                             \n",
       "...                                                                                                                                                                                          ...                             \n",
       "39077  t√©dio                                                                                                                                                                                                                 \n",
       "55345  q uminom                                                                                                                                                                                                              \n",
       "39161  voltar                                                                                                                                                                                                                \n",
       "88831  ir tentar dormir                                                                                                                                                                                                      \n",
       "60822  novos filmes s√©ries chegar netflix semana netflix netflixbrasil filmes s√©ries anima√ß√£o desenho anime document√°rio movie cinema sdv bolsonaro net youtube not√≠cia news assistir baixar lan√ßamentos estreias novidades  \n",
       "\n",
       "[811 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linhas duplicadas:\n",
      "811\n"
     ]
    }
   ],
   "source": [
    "# Verificando linhas duplicadas\n",
    "\n",
    "# Seleciona as linhas duplicadas no dataframe baseado em todas as colunas\n",
    "display(dfc[dfc.duplicated(keep='first')].sort_values(by=list(dfc.columns)))\n",
    "\n",
    "print(\"\\nLinhas duplicadas:\")\n",
    "print(dfc[dfc.duplicated()].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0cef9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclus√µes:** \n",
    "\n",
    "Ap√≥s tratamento do dataframe identificamos que diversos texto possuem as mesmas palavras e como o percentual de duplicidade √© baixo optamos em retirar as linhas repetidas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551a193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "832514dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando os dataframes\n",
    "\n",
    "# Criando copia do dataframe tratado para manter todos os registros \n",
    "dfc_copy = dfc.copy()\n",
    "\n",
    "# Convertendo int 64 para 32\n",
    "dfc['sentiment'] = dfc['sentiment'].astype(np.int16)\n",
    "\n",
    "# Excluindo os linhas duplicadas\n",
    "dfc = dfc.drop_duplicates(keep='first').copy()\n",
    "dfc.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2217c6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe original - df\n",
      "Quantidade de linhas...........: 95000\n",
      "Quantidade de colunas..........: 5\n",
      "\n",
      "Dataframe tratado - dfc\n",
      "Quantidade de linhas...........: 93793\n",
      "Quantidade de linhas duplicadas: 0\n",
      "Quantidade de colunas..........: 3\n",
      "\n",
      "Percentual de registros retirados: 1.27%\n"
     ]
    }
   ],
   "source": [
    "# Quantidade de linhas e colunas\n",
    "\n",
    "qtlc, qtcc = dfc.shape\n",
    "\n",
    "# Quantidade de linhas duplicadas\n",
    "qtd, _ = dfc[dfc.duplicated()].shape\n",
    "\n",
    "\n",
    "print('Dataframe original - df')\n",
    "print(f'Quantidade de linhas...........: {qtl}')\n",
    "print(f'Quantidade de colunas..........: {qtc}')\n",
    "\n",
    "print('\\nDataframe tratado - dfc')\n",
    "print(f'Quantidade de linhas...........: {qtlc}')\n",
    "print(f'Quantidade de linhas duplicadas: {qtd}')\n",
    "print(f'Quantidade de colunas..........: {qtcc}')\n",
    "\n",
    "print (f'\\nPercentual de registros retirados: {(100  * (qtl-qtlc) / qtl):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b6787",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclus√µes:** \n",
    "\n",
    "- Tratamos a vari√°vel **tweet_text** para criar a vari√°vel auxiliar **filtered_words**, onde os textos foram submetidos a tratamentos para definir as palavras chaves de sentimento;<br>\n",
    "\n",
    "\n",
    "- Criamos a vari√°vel **join_f_words**, para concatenar as palavras do coluna **filtered_words**;<br>\n",
    "\n",
    "\n",
    "- Criamos a vari√°vel **normalize_words**, tratando a vari√°vel **tweet_text** utilizando a biblioteca **enelvo**;<br>\n",
    "\n",
    "\n",
    "- Criamos um novo dataframe **dfc**, retirando as vari√°veis **id, tweet_date, query_used**, pois  as vari√°veis **id, tweet_date, query_used** n√£o s√£o relevantes para a modelagem;<br>\n",
    "\n",
    "\n",
    "- Retiramos todos os registros que estavam duplicados;<br>\n",
    "\n",
    "\n",
    "Com esses tratamentos acreditamos que o novo dataframe esta melhor preparado para an√°lise explorat√≥ria de dados e modelagem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdcd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7682d5be",
   "metadata": {},
   "source": [
    "## 3. An√°lise explorat√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes do dataframe\n",
    "\n",
    "dfc.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listagem das primeiras linhas do dataframe\n",
    "\n",
    "dfc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cffc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14f4b52d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Analisando Sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de20fc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**sentiment**\n",
    "- sentiment: 0, se negativo; 1, se positivo; 2, se neutro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567c31a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Quantidade de sentimentos\n",
    "\n",
    "Counter(dfc['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b796df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Distribui√ß√£o de tweets\n",
    "\n",
    "temp = dfc.groupby('sentiment').count()['join_f_words'].reset_index().sort_values(by='join_f_words',ascending=False)\n",
    "temp.style.background_gradient(cmap=cmap_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72634775",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Distribui√ß√£o da variavel sentiment\n",
    "\n",
    "col = 'sentiment'\n",
    "\n",
    "total = len(dfc)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "g = sns.countplot(x=col, data=dfc, palette=colors)\n",
    "g.set_title(f\"Distribui√ß√£o da vari√°vel {col}\")\n",
    "g.set_xlabel(f\"{col}\")\n",
    "g.set_ylabel(\"Quantidade\")\n",
    "sizes=[]\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    sizes.append(height)\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format((height/total)*100),\n",
    "            ha=\"center\", fontsize=14) \n",
    "g.set_ylim(0, max(sizes) * 1.15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488bcc2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "**Conclus√µes:** \n",
    "\n",
    "Analisando a vari√°vel **sentiment** podemos constatar que o nosso dataframe cont√©m quantidades de tweets balanceados, diminuindo a tend√™ncia do modelo para algum dos resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84e207",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45009fc3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Analisando as palavras dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0abaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Palavras mais utilizadas nos textos\n",
    "\n",
    "dfc['list_words'] = dfc['join_f_words'].apply(lambda w:str(w).split())\n",
    "\n",
    "top = Counter([item for sublist in dfc['list_words'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Palavras','Quantidade']\n",
    "temp.style.background_gradient(cmap=cmap_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b58fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Analisando a quantidade de palavras mais utilizadas\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "g = sns.barplot(x='Quantidade', y='Palavras', data=temp, palette=colors)\n",
    "g.set_title(f\"As 20 palavras mais utilizadas nos textos\")\n",
    "g.set_xlabel(f\"Quantidade\")\n",
    "g.set_ylabel(\"Palavras\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83261e20",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Analisando a quantidade de palavras mais utilizadas\n",
    "\n",
    "fig = px.treemap(temp, path=['Palavras'], values='Quantidade',title='As 20 palavras mais utilizadas nos textos')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495101ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da8d21a4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Palavras mais utilizadas por Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9281a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Criando dataframes por sentimentos\n",
    "\n",
    "# negativo\n",
    "Negative_sent = dfc[dfc['sentiment']==0]\n",
    "Negative_sent.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# positivo\n",
    "Positive_sent = dfc[dfc['sentiment']==1]\n",
    "Positive_sent.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# neutro\n",
    "Neutral_sent = dfc[dfc['sentiment']==2]\n",
    "Neutral_sent.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31a8e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Lista auxiliar com todas as palavras do dataframe\n",
    "\n",
    "raw_text = [word for word_list in dfc['list_words'] for word in word_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626484d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# O processamento de cada instru√ß√£o leva em m√©dia 7 minutos, totalizando aproximadamente 21 minutos\n",
    "# em uma m√°quina i7 com 12 cores e 32 ram\n",
    "\n",
    "# As 20 palavras utilizadas apenas para o sentimento negativo\n",
    "Unique_Negative= words_unique(0, 20, raw_text)\n",
    "\n",
    "# As 20 palavras utilizadas apenas para o sentimento positivo\n",
    "Unique_Positive = words_unique(1, 20, raw_text)\n",
    "\n",
    "# As 20 palavras utilizadas apenas para o sentimento neutro\n",
    "Unique_Neutral= words_unique(2, 20, raw_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30c327",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Sentimentos Negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c7f6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Palavras mais utilizadas em sentimentos negativos\n",
    "\n",
    "top = Counter([item for sublist in Negative_sent['list_words'] for item in sublist])\n",
    "temp_negative = pd.DataFrame(top.most_common(20))\n",
    "temp_negative = temp_negative.iloc[1:,:]\n",
    "temp_negative.columns = ['Palavras','Quantidade']\n",
    "temp_negative.style.background_gradient(cmap='Reds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890d809",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta√ß√£o da quantidade de palavras mais utilizadas em sentimentos negativos\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "g = sns.barplot(x='Quantidade', y='Palavras', data=temp_negative, palette=colors)\n",
    "g.set_title(f\"Palavras mais utilizadas nos textos negativos\")\n",
    "g.set_xlabel(f\"Quantidade\")\n",
    "g.set_ylabel(\"Palavras\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afff34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta as palavras utilizadas apenas em sentimentos negativos\n",
    "\n",
    "print('As 20 palavras mais utilizadas apenas em Tweets negativos:')\n",
    "Unique_Negative.style.background_gradient(cmap='Reds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414b719",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# As 20 palavras utilizadas apenas para o sentimento negativos\n",
    "\n",
    "fig = px.treemap(Unique_Negative, path=['words'], values='count',title='As 20 palavras mais utilizadas apenas em Tweets negativos')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85d56c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d950de81",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Sentimentos Positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f27e52",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Palavras mais utilizadas em sentimentos positivos\n",
    "\n",
    "top = Counter([item for sublist in Positive_sent['list_words'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(20))\n",
    "temp_positive.columns = ['Palavras','Quantidade']\n",
    "temp_positive.style.background_gradient(cmap=cmap_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097beb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta√ß√£o da quantidade de palavras mais utilizadas em sentimentos positivos\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "g = sns.barplot(x='Quantidade', y='Palavras', data=temp_positive, palette=colors)\n",
    "g.set_title(f\"Palavras mais utilizadas nos textos positivos\")\n",
    "g.set_xlabel(f\"Quantidade\")\n",
    "g.set_ylabel(\"Palavras\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a152e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta as palavras utilizadas apenas em sentimentos positivos\n",
    "\n",
    "print('As 20 palavras mais utilizadas apenas em Tweets positivos:')\n",
    "Unique_Positive.style.background_gradient(cmap=cmap_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff9213",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta√ß√£o das palavras utilizadas em sentimentos positivos\n",
    "\n",
    "fig = px.treemap(Unique_Positive, path=['words'], values='count',title='As 20 palavras mais utilizadas apenas em Tweets positivos')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fdcb4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cef58502",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Sentimentos Neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118a20e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Palavras mais utilizadas em sentimentos neutros\n",
    "\n",
    "top = Counter([item for sublist in Neutral_sent['list_words'] for item in sublist])\n",
    "temp_neutral = pd.DataFrame(top.most_common(20))\n",
    "temp_neutral = temp_neutral.loc[1:,:]\n",
    "temp_neutral.columns = ['Palavras','Quantidade']\n",
    "temp_neutral.style.background_gradient(cmap=cmap_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ca74a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta√ß√£o da quantidade de palavras mais utilizadas em sentimentos neutros\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "g = sns.barplot(x='Quantidade', y='Palavras', data=temp_neutral, palette=colors)\n",
    "g.set_title(f\"Palavras mais utilizadas nos textos neutros\")\n",
    "g.set_xlabel(f\"Quantidade\")\n",
    "g.set_ylabel(\"Palavras\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d396ded",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta as palavras utilizadas apenas em sentimentos neutros\n",
    "\n",
    "print('As 20 palavras utilizadas apenas em Tweets neutros:')\n",
    "Unique_Neutral.style.background_gradient(cmap=cmap_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99c445",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apresenta√ß√£o das palavras utilizadas em sentimentos neutros\n",
    "\n",
    "fig = px.treemap(Unique_Neutral, path=['words'], values='count',title='As 20 palavras mais utilizadas apenas em Tweets neutros')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92502f5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c3377",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# WordCloud para Tweets negativos\n",
    "\n",
    "text = ' '.join(Negative_sent['join_f_words'])\n",
    "plot_wordcloud(text, title = 'WordCloud Tweets negativos', backcolor = 'white', clrmap = 'Reds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71ff72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# WordCloud para Tweets positivos\n",
    "\n",
    "text = ' '.join(Positive_sent['join_f_words'])\n",
    "plot_wordcloud(text, title = 'WordCloud Tweets positivos', backcolor = 'white', clrmap = 'Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fa566",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# WordCloud para Tweets neutros\n",
    "\n",
    "text = ' '.join(Neutral_sent['join_f_words'])\n",
    "plot_wordcloud(text, title = 'WordCloud Tweets neutros', backcolor = 'white', clrmap = 'PuBu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73fa2a9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "**Conclus√µes:** \n",
    "\n",
    "- Podemos ver que as mesmas palavras s√£o comuns nos tr√™s segmentos;\n",
    "\n",
    "\n",
    "- Isso √© interessante porque palavras como dont e cant s√£o mais de natureza negativa e palavras como lol s√£o mais de natureza positiva, o que significa que os nossos dados est√£o incorrectamente etiquetados, teremos mais informa√ß√µes sobre isso ap√≥s a an√°lise de N-grama;\n",
    "\n",
    "\n",
    "- Olhando para as palavras √∫nicas de cada sentimento,temos agora muito mais clareza sobre os dados, estas palavras √∫nicas s√£o determinantes para o sentimento dos tweets;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082c8a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72070112",
   "metadata": {},
   "source": [
    "## 4. Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb77b8",
   "metadata": {},
   "source": [
    "### Separando conjunto de dados Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['Num_words_text'] = dfc['join_f_words'].apply(lambda x:len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65486f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nossa base de dados tem 93.793 mil linhas e a quantidade de palavras dispon√≠veis nos textos √© muito grande. \n",
    "# Para evitar problemas de aloca√ß√£o de mem√≥ria e processamento dos textos e modelagem, iremos criar uma amostra com 70% da base:\n",
    "\n",
    "dfc = dfc.sample(frac=0.7, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos dividir os nossos dados numa matriz X que cont√©m as caracter√≠sticas a treinar, \n",
    "# e uma matriz y com a vari√°vel alvo, neste caso a coluna covid_res. \n",
    "\n",
    "X = dfc['join_f_words']\n",
    "y = dfc['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86beb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos dividir os dados num conjunto de Train e num conjunto de Test. \n",
    "# Iremos treinar o modelo no conjunto de treino e depois utilizaremos o conjunto de testes para avaliar o modelo\n",
    "\n",
    "random_seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade total da vari√°vel \"target\" (sentiment)\n",
    "\n",
    "y.value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a27f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade separada para o conjunto de treino inicial\n",
    "\n",
    "y_train.value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade separada para o conjunto de teste inicial\n",
    "\n",
    "y_test.value_counts().sort_index()#(normalize = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8f1aa",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d833f",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac91f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9df319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo com os conjuntos de dados de treinamento \n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train).toarray()\n",
    "X_test_cv = cv.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de modelos\n",
    "\n",
    "list_models = [\n",
    "{'model_name': 'XGBClassifier',\n",
    " 'estimator' : XGBClassifier(n_jobs = 1, objective = 'multi:softmax', tree_method='approx', random_state = random_seed)},\n",
    "{'model_name': 'XGBClassifier',\n",
    " 'estimator' : XGBClassifier(n_jobs = 1, tree_method='approx', metric='multiclass', eval_metric='mlogloss', random_state = random_seed)}\n",
    "#{'model_name': 'Logistic Regression',\n",
    "# 'estimator' : LogisticRegression(random_state = random_seed, solver = 'lbfgs')}\n",
    "#{'model_name': 'Decision Tree',\n",
    "# 'estimator' : DecisionTreeClassifier(random_state = random_seed)}\n",
    "#,{'model_name': 'Random Forest',\n",
    "# 'estimator' : RandomForestClassifier(random_state = random_seed)}\n",
    "#,{'model_name': 'AdaBoost',\n",
    "# 'estimator' : AdaBoostClassifier(random_state = random_seed)}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcc793",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Processando os modelos baseado na list_models\n",
    "\n",
    "test_models_plot_roc_auc_curve(list_models,\n",
    "                              'model_name',\n",
    "                              'estimator',\n",
    "                              X_train_cv,\n",
    "                              X_test_cv,\n",
    "                              y_train,\n",
    "                              y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef0d7f",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciando TF-IDF\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo com os conjuntos de dados de treinamento \n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).todense()\n",
    "X_test_tfidf  = tfidf.transform(X_test).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processando os modelos baseado na list_models\n",
    "\n",
    "test_models_plot_roc_auc_curve(list_models,\n",
    "                               'model_name',\n",
    "                               'estimator',\n",
    "                               X_train_tfidf,\n",
    "                               X_test_tfidf,\n",
    "                               y_train,\n",
    "                               y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07e009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9646d853",
   "metadata": {},
   "source": [
    "## 5. Conclus√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c7950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6114f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
