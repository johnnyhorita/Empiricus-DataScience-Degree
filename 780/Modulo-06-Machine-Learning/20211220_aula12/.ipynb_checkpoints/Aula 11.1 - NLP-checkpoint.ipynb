{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 11 - Processamento de Linguagem Natural\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Dados Estruturados e Não Estruturados\n",
    "- 2) Introdução a NLP\n",
    "- 3) Processamento de Textos\n",
    "- 4) Exercícios\n",
    "- 5) Curva ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i1.wp.com/thedatascientist.com/wp-content/uploads/2018/09/data_science_wordcloud.png?fit=1584%2C1008&ssl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados Estruturados e Não-Estruturados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, precisamos entender qual a diferença enre as duas fontes de dados mais comuns, sendo elas dados **estruturados** e **não estruturados**. Definimos ele como:\n",
    "<br><br>\n",
    "- **Dados Estruturados:** São dados que seguem uma estrutura mais rígida com um padrão fixo e constante. Por exemplo: Tabelas e DataFrames;<br><br>\n",
    "- **Dados Não estruturados:** Como já diz o nome, são dados que não tem uma estrutura bem estabelecida e necessitam de um processamento adicional para trabalharmos com eles. Exemplos: áudios, vídeos, textos e etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução ao Processamento de Linguagem Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Processamento de Linguagem Natural, mas conhecido como NLP, é a abordagem onde trabalhamos com **dados não estruturados** do tipo **Texto**. O objetivo de trabalharmos com textos é extrair de informação e teor linguístico das nossas bases de textos e converter isso de uma forma númerica, onde poderemos utilizar em nossos modelos de *Machine Learning*.<br><br>\n",
    "Temos como exemplos de aplicações de NLP como:\n",
    "- Análise de Sentimentos em review de filmes e produtos ou mensagens em redes sociais;\n",
    "- Filtro de E-Mails Spams e Não Spams;\n",
    "- Identificação de textos a partir de construções linguísticas (descobrir se um texto foi escrito ou não por Machado de assis);\n",
    "- Tradutores de Idiomas;\n",
    "- ChatBots;\n",
    "- Corretores Ortográficos;\n",
    "- Classificação de textos de acordo com o conteúdo do texto (Esportes, Política, Economia e etc).\n",
    "<br><br>\n",
    "Nesta aula iremos aprender a partir dos nossos dados textuais a como processar, tratar e transformar os dados de uma maneira que os modelos de *Machine Learning* entendam.<br><br>\n",
    "\n",
    "A principal biblioteca de referência para NLP chama-se [NLTK - Natural Language Toll Kit](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de mais nada, precisamos filtrar e tratar os nossos textos, de forma a deixar apenas o conteúdo de mais relevantes para a nossa análise. Existem alguns processos importantes para trabalhar com os textos (não necessariamente você precisa aplicar todos os procesos!), onde iremos detalhar a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords são palavras que aparecem com uma frequência muito alta nos textos, mas que não trazem um teor de conteúdo relevante para o nosso modelo. Vamos entender isso na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ITX\n",
      "[nltk_data]     Gamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ITX\n",
      "[nltk_data]     Gamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixada a função de Stopwords, vamos definir um set de stopwords onde teremos uma lista com todas as stopwords em inglês já identificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora aplicar a remoção de Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Lista original é:  ['my', 'house', 'is', 'black', 'and', 'white', 'but', \"isn't\", 'big']\n",
      "A Lista sem stopwords é:  ['house', 'black', 'white', 'big']\n"
     ]
    }
   ],
   "source": [
    "example = [\"my\", \"house\", \"is\", \"black\", \"and\", \"white\", \"but\", \"isn't\", \"big\"]\n",
    "\n",
    "clean_list = []\n",
    "\n",
    "for word in example:\n",
    "    if word not in stopwords:\n",
    "        clean_list.append(word)\n",
    "        \n",
    "print('A Lista original é: ', example)\n",
    "print('A Lista sem stopwords é: ', clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza do Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem alguns cuidados com relação a grafia das palavras e elementos em um texto que devemos tomar bastante cuidado antes de fazer qualquer outra coisa. Esses pontos são:<br><br>\n",
    "- Transformar todas as palavras para MAIÚSCULAS ou minúsculas;\n",
    "- Remover caracteres especiais;\n",
    "- Remover dígitos (quando não forem relevantes);\n",
    "- Remover acentuação (caso típico de quando trabalhamos com textos em Português);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converter entre MAIÚSCULA e minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra Original:  OtoRriNoLaRINgOLoGIsTa\n",
      "Palavra Maiúscula:  OTORRINOLARINGOLOGISTA\n",
      "Palavra Minúscula:  otorrinolaringologista\n"
     ]
    }
   ],
   "source": [
    "string = 'OtoRriNoLaRINgOLoGIsTa'\n",
    "\n",
    "string_upper = string.upper()\n",
    "string_lower = string.lower()\n",
    "\n",
    "print('Palavra Original: ', string)\n",
    "print('Palavra Maiúscula: ', string_upper)\n",
    "print('Palavra Minúscula: ', string_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de dígitos, caracteres especiais e qualquer outro item que não queremos no texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para essa etapa do processo, iremos utilizar uma biblioteca auxiliar [RegEx (Regular Expression)](https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importada a biblioteca, vamos utilizar a função *re.sub*, para substituir os elementos que não queremos nos nossos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:  Siga nas redes sociais o @letscode, ja somos mais de 1 milhao de #hashtags e 200 mil followers\n",
      "Texto sem os números:  Siga nas redes sociais o @letscode, ja somos mais de  milhao de #hashtags e  mil followers\n",
      "Texto sem os caracteres especiais:  Siga nas redes sociais o letscode ja somos mais de milhao de hashtags e mil followers\n"
     ]
    }
   ],
   "source": [
    "string = 'Siga nas redes sociais o @letscode, ja somos mais de 1 milhao de #hashtags e 200 mil followers'\n",
    "print('Texto Original: ', string)\n",
    "\n",
    "# Remove Numbers\n",
    "string = re.sub(r'\\d', '', string)\n",
    "print('Texto sem os números: ', string)\n",
    "\n",
    "###\n",
    "# Remove Special Characters\n",
    "string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "print('Texto sem os caracteres especiais: ', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizem a documentação para descobrir mais códigos para filtrar elementos ou mesmo deem uma olhada nesse artigo, que resume de uma forma bem visual as aplicações do RegEx: [clique aqui](https://amitness.com/regex/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de Acentuação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a remoção de acentuação, iremos utilizar uma bibloteca chamada *Unidecode*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso precise instalar a biblioteca, descomente o código abaixo\n",
    "#!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "João Sebastião Alvará Vovô Linguiça Expressão\n",
      "Joao Sebastiao Alvara Vovo Linguica Expressao\n"
     ]
    }
   ],
   "source": [
    "string = 'João Sebastião Alvará Vovô Linguiça Expressão'\n",
    "print(string)\n",
    "\n",
    "string = unidecode(string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenização é um processo onde transformamos um texto de uma strin única em fragmentos desse texto na forma de *tokens*, que nada mais são do que as próprias palavras! Para isso, vamos utilizar a função *word_tokenize* do NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A frase orginal é:  O rato roeu a roupa do rei de Roma\n",
      "Os tokens são:  ['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'rei', 'de', 'Roma']\n"
     ]
    }
   ],
   "source": [
    "string = 'O rato roeu a roupa do rei de Roma'\n",
    "\n",
    "words = word_tokenize(string)\n",
    "\n",
    "print('A frase orginal é: ', string)\n",
    "print('Os tokens são: ', words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização de Textos (Text Normalization)** é o procedimento que consiste em **padronizar** o texto, de modo a evitar que variações tornem os modelos demasiadamente complexos. Por exemplo: tratar singular/plural como a mesma coisa, ou então eliminar conjugação de verbos. Outras componentes comuns da normalização são a de eliminar palavras que não agregam muito significado, ou palavras muito raras.\n",
    "\n",
    "Abaixo alguns exemplos de ações de Text Normalization que podem ser aplicadas no pré-processamento de dados textuais:\n",
    "\n",
    "**Stemming** - Redução de tokens à sua raiz invariante através da **remoção de prefixos ou sufixos**. Baseado em heurística<br>\n",
    "**Lemmatization** - Redução de tokens à sua raiz invariante através da **análise linguística do token**. Baseado em dicionário léxico<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say', 'write', 'run', 'ate', 'work']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['saying', 'writing', 'running', 'ate', 'worked']\n",
    "\n",
    "stem_words = []\n",
    "for w in words:\n",
    "    s_words = stemmer.stem(w)\n",
    "    stem_words.append(s_words)\n",
    "    \n",
    "stem_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\ITX\n",
      "[nltk_data]     Gamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# na primeira vez, é necessário baixar o wordnet\n",
    "# após a primeira vez, pode comentar a linha abaixo\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "running : run\n",
      "went : go\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "\n",
    "# argumento \"pos\" indica a qual classe gramatical o token pertence\n",
    "print(\"running :\", lemmatizer.lemmatize(\"running\", pos = \"v\")) \n",
    "print(\"went :\", lemmatizer.lemmatize(\"went\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de Processamento de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conhecendo todos os tipos de processamentos que podemos utilizar, uma forma útil e organizada para isso é construirmos uma funçãi que receba o nosso dados originais e realizada todos os processamentos que queremos nos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline - Text Preprocessing\n",
    "def preprocessing(string):\n",
    "    ###\n",
    "    # Remove Numbers\n",
    "    string = re.sub(r'\\d', '', string)\n",
    "    ###\n",
    "    # Remove Special Characters\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    ###\n",
    "    # Lowercase words\n",
    "    string = string.lower()\n",
    "    ###\n",
    "    # Word Tokenize\n",
    "    words = word_tokenize(string)\n",
    "    ###\n",
    "    # Remove Stopwords\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            filtered_words.append(w)\n",
    "    ###\n",
    "    # Stemming Words\n",
    "    stem_words = []\n",
    "    for w in filtered_words:\n",
    "        s_words = stemmer.stem(w)\n",
    "        stem_words.append(s_words)\n",
    "    ###\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora já começar a práticar com os nossos dados de exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('./datasets/movies.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso exemplo será uma Análise de Sentimento em Críticas de Filmes, onde vamos identificar se a crítica foi boa ou não:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    50000 non-null  object\n",
      " 1   label   50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nossa base de dados tem 50 mil linhas e levando em consideração que as críticas são sobre filmes diversos, a quantidade de palavras disponíveis nos textos será muito grande. Para economizar tempo de aula com processamneto dos textos e modelagem, iremos criar uma amostra com 10% da base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_sample = movies.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos aplicar o nosso processamento dos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "# Normalmente depois do processamento juntamos as palavras novamente em uma só string\n",
    "\n",
    "movies_sample['join_words'] = movies_sample['filtered_words'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>filtered_words</th>\n",
       "      <th>join_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>I really liked the Far Cry game, nice graphics...</td>\n",
       "      <td>0</td>\n",
       "      <td>[realli, like, far, cri, game, nice, graphic, ...</td>\n",
       "      <td>realli like far cri game nice graphic good lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35928</th>\n",
       "      <td>An unflinching descent into psychological and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[unflinch, descent, psycholog, physic, oblivio...</td>\n",
       "      <td>unflinch descent psycholog physic oblivion und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957</th>\n",
       "      <td>I expected a comedy like the \"Big Mama\" movies...</td>\n",
       "      <td>0</td>\n",
       "      <td>[expect, comedi, like, big, mama, movi, instea...</td>\n",
       "      <td>expect comedi like big mama movi instead movi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36191</th>\n",
       "      <td>The Toxic Avenger... &lt;br /&gt;&lt;br /&gt;The idea of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[toxic, aveng, br, br, idea, movi, person, com...</td>\n",
       "      <td>toxic aveng br br idea movi person common popu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39386</th>\n",
       "      <td>This is possibly the worst of the cockney gang...</td>\n",
       "      <td>0</td>\n",
       "      <td>[possibl, worst, cockney, gangster, genr, blig...</td>\n",
       "      <td>possibl worst cockney gangster genr blight bri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "758    I really liked the Far Cry game, nice graphics...      0   \n",
       "35928  An unflinching descent into psychological and ...      1   \n",
       "16957  I expected a comedy like the \"Big Mama\" movies...      0   \n",
       "36191  The Toxic Avenger... <br /><br />The idea of t...      0   \n",
       "39386  This is possibly the worst of the cockney gang...      0   \n",
       "\n",
       "                                          filtered_words  \\\n",
       "758    [realli, like, far, cri, game, nice, graphic, ...   \n",
       "35928  [unflinch, descent, psycholog, physic, oblivio...   \n",
       "16957  [expect, comedi, like, big, mama, movi, instea...   \n",
       "36191  [toxic, aveng, br, br, idea, movi, person, com...   \n",
       "39386  [possibl, worst, cockney, gangster, genr, blig...   \n",
       "\n",
       "                                              join_words  \n",
       "758    realli like far cri game nice graphic good lev...  \n",
       "35928  unflinch descent psycholog physic oblivion und...  \n",
       "16957  expect comedi like big mama movi instead movi ...  \n",
       "36191  toxic aveng br br idea movi person common popu...  \n",
       "39386  possibl worst cockney gangster genr blight bri...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso chegamos ao fim do pré-processamento, uma das etapas mais importantes de todo projeto de NLP!\n",
    "\n",
    "É importante ressaltar que a escolha das etapas de pré-processamento não é algo óbvio, dado que há muitas escolhas possíveis acerca do que se fazer para pré-processar os dados. Assim, o indicado é treinar diferentes modelos testando diferentes combinações das técnicas de pré-processamento, até que o melhor procedimento seja encontrado!\n",
    "\n",
    "**Nomenclatura**: o conjunto de mensagens pré-processadas é chamado de **Corpus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulário\n",
    "\n",
    "O vocabulário do corpus nada mais é do que uma listagem das palavras individuais que aparecem no corpus. Para encontrar o vocabulário, basta contarmos a aparição de cada palavra isolada no corpus. Ao fim, teremos N palavras únicas que compõem nosso vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = []\n",
    "for frase in movies_sample['join_words']:\n",
    "    for palavra in frase.split():\n",
    "        \n",
    "        #não queremos palavras de uma única letra (pode acontecer devido ao stemming...)\n",
    "        if len(palavra) > 1:\n",
    "            if palavra not in [x[0] for x in vocabulario]:\n",
    "                vocabulario.append([palavra, 1])\n",
    "            else:\n",
    "                vocabulario[[x[0] for x in vocabulario].index(palavra)][1] += 1\n",
    "            \n",
    "print(\"\\nO vocabulário é formado por N =\", len(vocabulario), \"palavras!\")\n",
    "\n",
    "#a partir do vocabulário, crio um dataframe com a contagem\n",
    "vocab_count = pd.DataFrame({\"palavra\": [],\n",
    "                            \"count\": []})\n",
    "\n",
    "vocab_count[\"palavra\"] = pd.Series(vocabulario).apply(lambda x: x[0])\n",
    "vocab_count[\"count\"] = pd.Series(vocabulario).apply(lambda x: x[1])\n",
    "vocab_count = vocab_count.sort_values(\"count\", ascending=False)\n",
    "\n",
    "print(\"\\nTemos a seguir as 10 mais comuns, com as respectivas contagens:\")\n",
    "display(vocab_count.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Usando a base *spamham.csv*, faça o processamento dos textos aplicando as limpezas necessárias para tal. Tente levantar o vocabulário dos e-mails e print o top 10 palavras deste dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Utilizando os dados de tweets vamos avaliar  tweets são de desastres ou não. Essa base é um dataset conhecido do Kaggle, onde vocês podem ter mais detalhes [clicando aqui](https://www.kaggle.com/c/nlp-getting-started/overview). Faça o processamento dos textos aplicando as limpezas necessárias para tal. Tente levantar o vocabulário dos e-mails e print o top 10 palavras deste dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
